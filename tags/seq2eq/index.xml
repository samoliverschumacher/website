<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>seq2eq on Sam Schumacher</title>
    <link>/tags/seq2eq/</link>
    <description>Recent content in seq2eq on Sam Schumacher</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 08 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/seq2eq/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Surgery on an Attentional Neural Network</title>
      <link>/post/attnrnnlabotomy/</link>
      <pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/attnrnnlabotomy/</guid>
      <description>Why this project?  Understand &amp;lsquo;Attention&amp;rsquo;, in Natural Language Processing algorithms. Advancing explainability of A.I. Understand the flexible nature of Neural Networks   Table of Contents  Why this project?  Table of Contents   Normal use of Attention Concept of Attention Recap on RNNs Combining RNNs with Attn. Purpose of project Project Specs, Setup &amp;amp; modelling process 3 Model Topologies  LSTM w. Attn. BiLSTM w. Attn. Forgetful Encoder LSTM w.</description>
    </item>
    
  </channel>
</rss>
