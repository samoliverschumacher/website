<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Sam Schumacher</title>
    <link>/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on Sam Schumacher</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Socratic argument against using Validation Sets</title>
      <link>/post/socratesvalidation/</link>
      <pubDate>Thu, 20 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/socratesvalidation/</guid>
      <description>&lt;p&gt;&lt;em&gt;There is no real utility in taking the position that you &lt;em&gt;&lt;strong&gt;won&amp;rsquo;t do model selection via validation splits&lt;/strong&gt;&lt;/em&gt;, although, know there is a meta-purpose to bringing a blowtorch to this concept&amp;hellip;&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>5-min reads: why propogation cell state through mini-batches slows convergence</title>
      <link>/post/rnn5min_1/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/rnn5min_1/</guid>
      <description>&lt;p&gt;&lt;em&gt;In this 5-min read I&amp;rsquo;ll try to explain how convergence can be slower for LSTM neural networks where memory state is maintained from one mini-batch to the next. An example of this problem can be found in my &lt;a href=&#34;../../post/eeg_modelling/&#34; title=&#34;Autonomous vehicle microsleep detector&#34;&gt;project about a microsleep detector&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Autonomous vechicle microsleep detector Vol3: Misclassification tradeoff</title>
      <link>/post/eeg_costtradeoff/</link>
      <pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/eeg_costtradeoff/</guid>
      <description>&lt;p&gt;&lt;em&gt;It&amp;rsquo;s much more costly to the end user for the autonomous vehcile to not take over the steering when eyes are closed, than to take over steering when eyes are open.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Autonomous vechicle microsleep detector Vol2: Modelling</title>
      <link>/post/eeg_modelling/</link>
      <pubDate>Sun, 20 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/eeg_modelling/</guid>
      <description>&lt;p&gt;&lt;em&gt;Autonomous vehicles are no longer reserved for the realm of science fiction, but some AI experts say the complexities of AI-driven cars in public make the possibility of never needing to drive a far off future. Perhaps to bridge the gap are technologies that greatly reduce the labour involved in driving, while still reducing the risk.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Autonomous vechicle microsleep detector Vol1: EDA</title>
      <link>/post/eeg_eda/</link>
      <pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/eeg_eda/</guid>
      <description>&lt;p&gt;&lt;em&gt;Produce a detector that distinguisihes when a car&amp;rsquo;s driver has eyes closed. This detector will connect to an autopilot system for the car temporarily until driver regains control, otherwise safely brings the car to a halt on the side of the road. Detector will have access to EEG signals connected to drivers head.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Surgery on an Attentional Neural Network</title>
      <link>/post/attnrnnlabotomy/</link>
      <pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/attnrnnlabotomy/</guid>
      <description>Why this project?  Understand &amp;lsquo;Attention&amp;rsquo;, in Natural Language Processing algorithms. Advancing explainability of A.I. Understand the flexible nature of Neural Networks   Table of Contents  Why this project?  Table of Contents   Normal use of Attention Concept of Attention Recap on RNNs Combining RNNs with Attn. Purpose of project Project Specs, Setup &amp;amp; modelling process 3 Model Topologies  LSTM w. Attn. BiLSTM w. Attn. Forgetful Encoder LSTM w.</description>
    </item>
    
    <item>
      <title>A DIY reccurent neural network algorithm</title>
      <link>/post/buildingnnet/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/buildingnnet/</guid>
      <description>See the LSTM building &amp;amp; training algorithm I developed as a learning excercise on my Github</description>
    </item>
    
  </channel>
</rss>
